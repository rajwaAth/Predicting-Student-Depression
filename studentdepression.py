# -*- coding: utf-8 -*-
"""StudentDepression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ph-oUADxD-8fgGfhY23Fn1yU4c26VlXz

# Predicting Student Depression

dataset: https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset

# Importing Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)

!pip install opendatasets
import opendatasets as od

"""# Load Dataset"""

od.download('https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset', force=True)

df = pd.read_csv('/content/student-depression-dataset/student_depression_dataset.csv')
df.head()

"""# Data Understanding"""

ListofDecs = []

for i in df.columns:
  ListofDecs.append([i, df[i].dtype, df.shape[0], df[i].isna().sum(), round((df[i].isna().sum()/df.shape[0])*100, 2), df[i].nunique(), df[i].unique()[:5]])

df_decs = pd.DataFrame(ListofDecs, columns=['column', 'type', 'total row', 'null', 'percentage null', 'number of unique value', 'sample unique value'])
df_decs

"""Kolom `CustomerID` hanya berisi nilai unik untuk setiap pelanggan dan **tidak memberikan kontribusi prediktif**, sehingga kolom ini akan **dihapus dari analisis**.

Selain itu terdapat pula kolom `Financial Stress` yang bertipe data **object** yang seharusnya bertipe data **float**, sehingga perlu dilakukan konversi tipe data sebelum analisis lebih lanjut.
"""

df = df.drop('id', axis=1)
df['Financial Stress'] = pd.to_numeric(df['Financial Stress'], errors='coerce')

"""# Exploratory Data Analysis

## Univariate Analysis
"""

cat = df.select_dtypes('object').columns
num = df.select_dtypes('number').columns.drop('Depression')

print(f'Kolom dengan tipe Kategori: \n{cat}\n')
print(f'Kolom dengan tipe Numerik: \n{num}')

import matplotlib.pyplot as plt

# Set figure
plt.figure(figsize=(8,8))

# Pie plot
plt.pie(
    df['Depression'].value_counts(),
    labels=df['Depression'].value_counts().index.map({0:'No ', 1:'Yes'}),
    autopct='%1.1f%%',
    textprops={'fontsize': 14, 'fontweight': 'bold'},
    colors=['#0786ff', '#ff9999'],
    startangle=90,
    explode=(0.05, 0),
)

# Judul
plt.title('Distribusi Depression', fontsize=18, fontweight='bold')

# Menjaga bentuk pie bulat
plt.axis('equal')

# Tampilkan plot
plt.show()

"""Distribusi pada data target (Depression) sedikit **imbalance** (tidak seimbang), yang mungkin dapat menyebabkan model cenderung memprediksi kelas mayoritas. Oleh karena itu, untuk mengantisipasi permasalahan ini, akan dilakukan percobaan menggunakan teknik **oversampling** pada **data train** (*setelah proses pembagian data menjadi data latih dan data uji*)."""

# Melakukan visualisasi countplot terhadap kolom kategori
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(cat):
  plt.subplot(2, 4, i+1)
  ax = sns.countplot(data=df, y=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

"""Berdasarkan distribusi nilai pada kolom-kolom kategorikal, ditemukan bahwa kolom `City` dan `Profession` memiliki beberapa kategori dengan jumlah data yang sangat sedikit (dominan pada satu kategori saja). Selain itu, kolom City juga memiliki terlalu banyak kategori, yang ***dapat menyebabkan curse of dimensionality***. Oleh karena itu, kedua kolom tersebut akan dihapus dari dataset.

Selain itu, pada kolom `Sleep Duration`, `Dietary Habits`, dan `Degree`, terdapat kategori bernilai "Others" yang tidak merepresentasikan informasi yang jelas serta jumlahnya sangat sedikit. Maka dari itu, baris data yang memiliki nilai "Others" pada fitur-fitur tersebut akan dihapus dari dataset.
"""

# Melakukan visualisasi histogram terhadap kolom numerik
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(2, 4, i+1)
  sns.histplot(data=df, x=col, color='blue', kde=True)
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

# Melakukan visualisasi boxplot terhadap kolom numerik
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(2, 4, i+1)
  sns.boxplot(data=df, x=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

""" Kolom `Work Pressure` dan `Job Satisfaction` juga menunjukkan dominasi pada satu nilai tertentu, sehingga tidak memberikan variasi yang signifikan untuk analisis. Oleh karena itu, kedua kolom tersebut akan dihapus dari dataset.  

 Sementara itu, kolom `Age` dan `CGPA` teridentifikasi memiliki nilai outlier yang dapat memengaruhi hasil analisis. Outlier pada kedua kolom tersebut akan dihapus pada tahap praproses selanjutnya.
"""

# menampilkan korelasi antara feature yg ada
plt.figure(figsize=(10,10))
sns.heatmap(df[num].corr(), annot=True, fmt='.2f')
plt.tight_layout()
plt.title('Correlation Between Feature Numeric', weight='bold')
plt.show()

df.drop(['City', 'Profession', 'Job Satisfaction', 'Work Pressure'], axis=1, inplace=True)

df = df[~((df['Sleep Duration'] == 'Others') |
        (df['Dietary Habits'] == 'Others') |
        (df['Degree'] == 'Others') |
        (df['Financial Stress'] == 7))]

df.shape

"""Apabila dilihat berdasarkan korelasi antara feature numerik yang dimiliki, pada dataset ini tidak terdapat feature redundant

# Multivariate Analysis
"""

def plot_distribution_percentage_category(col, orient):

  def percentage_feature(col):
    df_perce = df.groupby([col, 'Depression']).size().reset_index().rename(columns={0:'Count'})
    df_perce['Percentage'] = ((df_perce['Count']/df_perce.groupby(col)['Count'].transform('sum'))*100).round(2)
    df_perce.sort_values('Percentage', ascending= False, inplace=True)
    return df_perce

  data = percentage_feature(col)

  if orient == 'h':
    plt.figure(figsize=(12, 8))
    ax = sns.barplot(y=col, x='Percentage', hue='Depression', data=data, orient=orient, palette="mako")
    for p in ax.patches:
        width = p.get_width()
        ax.annotate(f'{width:.2f}%',
                    xy=(width / 2, p.get_y() + p.get_height() / 2),
                    ha='center', va='center', color='white', fontsize=10, fontweight='bold')

    plt.xlabel('Percentage by Total Customers', fontsize=12)

  else:
    plt.figure(figsize=(12, 8))
    ax =sns.barplot(data=data, x=col, y='Percentage', hue='Depression', palette='mako')
    for p in ax.patches:
      height = p.get_height()
      ax.annotate(f'{height:.2f}%',
                  xy=(p.get_x() + p.get_width() / 2, height/2),
                  ha='center', va='bottom', fontsize=10, color='white', fontweight='bold')

    plt.ylabel('Percentage by Total Customers', fontsize=12)

  plt.title(f"Percentage of Overall {col} Distribution", fontsize=14, fontweight='bold')
  plt.legend(title='Depression', fontsize=10, title_fontsize=12)
  plt.show()

plot_distribution_percentage_category('Gender', 'v')

plot_distribution_percentage_category('Sleep Duration', 'h')

plot_distribution_percentage_category('Dietary Habits', 'v')

plot_distribution_percentage_category('Degree', 'h')

plot_distribution_percentage_category('Have you ever had suicidal thoughts ?', 'v')

plot_distribution_percentage_category('Academic Pressure', 'v')

plot_distribution_percentage_category('Financial Stress', 'h')

def plot_distribution_numeric(data, col):
  plt.figure(figsize=(10,6))
  sns.histplot(data=data, x=col, hue='Depression', kde=True)

  mean_all = data[col].mean()
  mean_Depression = data[data['Depression'] == 1][col].mean()
  mean_not_Depression = data[data['Depression'] == 0][col].mean()

  plt.axvline(mean_all, color='#28B463', linestyle='dashed', label=f'Rata-rata keseluruhan {mean_all:.2f}')
  plt.axvline(mean_Depression, color='#2E86C1', linestyle='dotted', label=f'Rata-rata customer Depression {mean_Depression:.2f}')
  plt.axvline(mean_not_Depression, color='#FF5733', linestyle='solid', label=f'Rata-rata customer tidak Depression {mean_not_Depression:.2f}')

  plt.text(mean_all, plt.ylim()[1]*0.9, color='#28B463', s=f'Rata-rata keseluruhan\n{mean_all:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
  plt.text(mean_Depression, plt.ylim()[1]*0.8, color='#2E86C1', s=f'Rata-rata customer Depression\n{mean_Depression:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
  plt.text(mean_not_Depression, plt.ylim()[1]*0.7, color='#FF5733', s=f'Rata-rata customer tidak Depression\n{mean_not_Depression:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))

  plt.title(f'Distribusi {col} Berdasarkan Status Depression Customer', fontsize=14)
  plt.xlabel(f'{col}', fontsize=12)
  plt.ylabel('Jumlah Customer', fontsize=12)
  plt.tight_layout()
  plt.show()

plot_distribution_numeric(df, 'Age')

plot_distribution_numeric(df, 'CGPA')

plot_distribution_numeric(df, 'Study Satisfaction')

plot_distribution_numeric(df, 'Work/Study Hours')

"""# Data Preprocessing"""

df_prep = df.copy()

"""## Handle Missing Values"""

df_prep.isnull().sum()

df_prep = df_prep.dropna()

df_prep.isnull().sum()

"""## Handle Duplicate Values"""

# Handle Duplicate Value
(f'Jumlah Data Duplikat: {df_prep.duplicated().sum()}')

"""## Handle Outlier Values"""

col_selected = ['Age', 'CGPA']

def remove_outliers(df, columns):
    df_cleaned = df.copy()
    for col in columns:
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_cleaned = df_cleaned[~((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound))]
    return df_cleaned

df_prep = remove_outliers(df_prep, col_selected)

"""## Encoding Values"""

df_prep.head()

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Label Encoding untuk 'Gender'
col_label = ['Gender', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']
for col in col_label:
  df_prep[col] = LabelEncoder().fit_transform(df_prep[col])

# One Hot Encoding untuk kolom yang disebutkan
one_hot_columns =  ['Sleep Duration', 'Dietary Habits', 'Degree']
df_prep = pd.get_dummies(df_prep, columns=one_hot_columns, dtype=int)

df_prep.head()

df_prep.shape

"""## Spliting Data"""

from sklearn.model_selection import train_test_split

X = df_prep.drop('Depression', axis=1)
y = df_prep['Depression']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')

"""## Transformation Values"""

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""## Handle Imbalance Data"""

print(f'Number of Depression Before SMOTE: {y_train.value_counts()[1]}')
print(f'Number of Not Depression Loan Before SMOTE: {y_train.value_counts()[0]}')

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_smt, y_train_smt = smote.fit_resample(X_train, y_train)

print(f'Number of Depression After SMOTE: {y_train_smt.value_counts()[1]}')
print(f'Number of Not Depression After SMOTE: {y_train_smt.value_counts()[0]}')

"""# Modeling"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

def train_model(X_train, y_train, X_test, y_test):
  # list model
  models ={
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Ada Boost': AdaBoostClassifier(random_state=42),
    'SVC': SVC(random_state=42),
    'KNeigbors': KNeighborsClassifier(),
    'Gaussian NB': GaussianNB(),
    'XGBoost': XGBClassifier(random_state=42)
    }


  result = []
  for nama_model, model in models.items():
    model.fit(X_train, y_train)
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)


    # data train
    acc_train = round(accuracy_score(y_train, y_pred_train),2)
    prec_train = round(precision_score(y_train, y_pred_train),2)
    rec_train = round(recall_score(y_train, y_pred_train),2)
    f1_train = round(f1_score(y_train, y_pred_train),2)
    roc_train = round(roc_auc_score(y_train, y_pred_train),2)

    # data test
    acc_test = round(accuracy_score(y_test, y_pred_test),2)
    prec_test = round(precision_score(y_test, y_pred_test),2)
    rec_test = round(recall_score(y_test, y_pred_test),2)
    f1_test = round(f1_score(y_test, y_pred_test),2)
    roc_test = round(roc_auc_score(y_test, y_pred_test),2)

    # result
    result.append([nama_model, acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
  result_df = pd.DataFrame(result, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
  return result_df

result = train_model(X_train_smt, y_train_smt, X_test, y_test)
print('With SMOTE')
result

result = train_model(X_train, y_train, X_test, y_test)
print('Without SMOTE')
result

"""## XGBoost & Hyperparamater Tuning"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Parameter grid untuk XGBoost
param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [1, 1.5, 2]
}

# Inisialisasi XGBoost
xgb = XGBClassifier(eval_metric='logloss', random_state=42)

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)

# Fit model
random_search.fit(X_train, y_train)

# Parameter terbaik
best_params_XG = random_search.best_params_
print(f"Best parameters: {best_params_XG}")

# Model dengan parameter terbaik
model_tuning_XG = random_search.best_estimator_
model_tuning_XG.fit(X_train, y_train)

# Prediksi
y_pred_test = model_tuning_XG.predict(X_test)
y_pred_train = model_tuning_XG.predict(X_train)

# Evaluasi train
acc_train = round(accuracy_score(y_train, y_pred_train), 2)
prec_train = round(precision_score(y_train, y_pred_train), 2)
rec_train = round(recall_score(y_train, y_pred_train), 2)
f1_train = round(f1_score(y_train, y_pred_train), 2)
roc_train = round(roc_auc_score(y_train, y_pred_train), 2)

# Evaluasi test
acc_test = round(accuracy_score(y_test, y_pred_test), 2)
prec_test = round(precision_score(y_test, y_pred_test), 2)
rec_test = round(recall_score(y_test, y_pred_test), 2)
f1_test = round(f1_score(y_test, y_pred_test), 2)
roc_test = round(roc_auc_score(y_test, y_pred_test), 2)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix XGBoost (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Hasil akhir
result_tuning = []
result_tuning.append(['XGBoost(Tuning)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_tuning = pd.DataFrame(result_tuning, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_tuning

from sklearn.model_selection import cross_validate
import numpy as np

# Melakukan cross validation pada model terbaik dari RandomizedSearch
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

cv_results = cross_validate(
    model_tuning_XG,           # Model terbaik hasil tuning
    X_test,                # Data training
    y_test,                # Label training
    cv=5,                   # 5-fold CV
    scoring=scoring,
    return_train_score=True,
    n_jobs=-1
)

# Tampilkan hasil rata-rata CV
print("Cross Validation Scores (mean of 5 folds):")
for metric in scoring:
    mean_score = np.mean(cv_results[f'test_{metric}'])
    print(f"{metric}: {mean_score:.2f}")

"""## Logistic Regressio & Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = [
    # liblinear: hanya l1 dan l2
    {
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'max_iter': [100, 200, 500, 1000],
    },
    # lbfgs: hanya l2 dan None
    {
        'penalty': ['l2', None],
        'solver': ['lbfgs'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'max_iter': [100, 200, 500, 1000],
    },
    # saga: bisa l1, l2, elasticnet, None
    {
        'penalty': ['l1', 'l2', 'elasticnet', None],
        'solver': ['saga'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'l1_ratio': [0.1, 0.5, 0.9],  # hanya dipakai kalau penalty == 'elasticnet'
        'max_iter': [100, 200, 500, 1000],
    },
]

# Initialize Logistic Regression
Logistic = LogisticRegression(random_state=42)

# Initialize GridSearchCV
random_search = RandomizedSearchCV(
    estimator=Logistic,
    param_distributions=param_grid,
    n_iter=10,               # Jumlah iterasi (kombinasi) untuk dicoba
    cv=3,                    # Cross-validation 5 fold
    scoring='roc_auc',       # Metode evaluasi ROC AUC
    n_jobs=-1,               # Paralel untuk mempercepat
    random_state=42          # Seed untuk hasil yang konsisten
)

# Fit the grid search to the data
random_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params_LR = random_search.best_params_

print(f"Best parameters: {best_params_LR}")

# Train the model with the best parameters
model_tuning_LR = random_search.best_estimator_
model_tuning_LR.fit(X_train, y_train)

# Evaluate the model (example using roc_auc_score)
result_tuning = []

y_pred_test = model_tuning_LR.predict(X_test)
y_pred_train = model_tuning_LR.predict(X_train)

# data train
acc_train = round(accuracy_score(y_train, y_pred_train),2)
prec_train = round(precision_score(y_train, y_pred_train),2)
rec_train = round(recall_score(y_train, y_pred_train),2)
f1_train = round(f1_score(y_train, y_pred_train),2)
roc_train = round(roc_auc_score(y_train, y_pred_train),2)

# data test
acc_test = round(accuracy_score(y_test, y_pred_test),2)
prec_test = round(precision_score(y_test, y_pred_test),2)
rec_test = round(recall_score(y_test, y_pred_test),2)
f1_test = round(f1_score(y_test, y_pred_test),2)
roc_test = round(roc_auc_score(y_test, y_pred_test),2)

# showing confusion metrics
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix Logistic Regression (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

result_tuning.append(['LogisticRegression(Tuning)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_tuning = pd.DataFrame(result_tuning, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_tuning

from sklearn.model_selection import cross_validate
import numpy as np

# Melakukan cross validation pada model terbaik dari RandomizedSearch
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

cv_results = cross_validate(
    model_tuning_LR,           # Model terbaik hasil tuning
    X_test,                # Data training
    y_test,                # Label training
    cv=5,                   # 5-fold CV
    scoring=scoring,
    return_train_score=True,
    n_jobs=-1
)

# Tampilkan hasil rata-rata CV
print("Cross Validation Scores (mean of 5 folds):")
for metric in scoring:
    mean_score = np.mean(cv_results[f'test_{metric}'])
    print(f"{metric}: {mean_score:.2f}")

"""# Feature Importance"""

import shap

# Assuming 'model_tuning_XG' is your best XGBoost model
explainer = shap.Explainer(model_tuning_XG, X_train)
shap_values = explainer(X_test)

# Visualize feature importance
shap.summary_plot(shap_values, X_test, plot_type="bar", feature_names=X.columns, max_display=7)
shap.summary_plot(shap_values, X_test, feature_names=X.columns, max_display=7)
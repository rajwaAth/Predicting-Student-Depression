# -*- coding: utf-8 -*-
"""StudentDepression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ph-oUADxD-8fgGfhY23Fn1yU4c26VlXz

# Predicting Student Depression

**Submission 1 - Machine Learning Terapan**

Nama: Muhamad Rajwa Athoriq

dataset: https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset

## Latar Belakang

Kesehatan mental merupakan aspek penting dalam mendukung keberhasilan akademik dan kehidupan sosial siswa maupun mahasiswa. Dalam beberapa tahun terakhir, terdapat peningkatan signifikan terhadap kasus gangguan mental, khususnya depresi, di kalangan pelajar. Organisasi Kesehatan Dunia (WHO) melaporkan bahwa lebih dari 264 juta orang di dunia menderita depresi, dan sebagian besar kasus dimulai pada usia remaja dan dewasa muda – usia yang identik dengan masa sekolah dan kuliah (World Health Organization, 2020).

Dalam konteks pendidikan tinggi, tekanan akademik, ekspektasi sosial, kesepian, gaya hidup yang tidak seimbang, dan masalah finansial merupakan pemicu utama gangguan psikologis. Menurut sebuah studi oleh American College Health Association (2021), lebih dari 40% mahasiswa di Amerika Serikat melaporkan mengalami gejala depresi dalam satu tahun terakhir. Hal serupa juga terjadi di Indonesia, sebagaimana diungkapkan oleh Kementerian Kesehatan RI, bahwa sekitar 6,1% remaja usia 15–24 tahun mengalami gangguan mental emosional, dan angka ini cenderung meningkat setiap tahunnya (Riskesdas, 2018).

Meningkatnya prevalensi depresi pada pelajar menuntut adanya solusi berbasis teknologi untuk mengidentifikasi risiko secara dini. Namun, pendekatan konvensional seperti konseling tatap muka atau survei manual sering kali bersifat reaktif, tidak efisien, dan sulit menjangkau semua siswa secara merata. Oleh karena itu, diperlukan pendekatan yang lebih sistematis dan prediktif dengan memanfaatkan machine learning dan analitik data.

# Importing Library
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)

!pip install opendatasets
import opendatasets as od

"""# Load Dataset"""

od.download('https://www.kaggle.com/datasets/adilshamim8/student-depression-dataset', force=True)

df = pd.read_csv('/content/student-depression-dataset/student_depression_dataset.csv')
df.head()

"""# Data Understanding"""

ListofDecs = []

for i in df.columns:
  ListofDecs.append([i, df[i].dtype, df.shape[0], df[i].isna().sum(), round((df[i].isna().sum()/df.shape[0])*100, 2), df[i].nunique(), df[i].unique()[:5]])

df_decs = pd.DataFrame(ListofDecs, columns=['column', 'type', 'total row', 'null', 'percentage null', 'number of unique value', 'sample unique value'])
df_decs

"""Kolom `CustomerID` hanya berisi nilai unik untuk setiap pelanggan dan **tidak memberikan kontribusi prediktif**, sehingga kolom ini akan **dihapus dari analisis**.

Selain itu terdapat pula kolom `Financial Stress` yang bertipe data **object** yang seharusnya bertipe data **float**, sehingga perlu dilakukan konversi tipe data sebelum analisis lebih lanjut.

#### Menghapus Kolom yang Tidak diperlukan & Merubah Tipe Data
"""

# menghapus kolom id dan mengubah tipe data pada kolom financial stress
df = df.drop('id', axis=1)
df['Financial Stress'] = pd.to_numeric(df['Financial Stress'], errors='coerce')

# lakukan deskripsi data ulang
ListofDecs = []

for i in df.columns:
  ListofDecs.append([i, df[i].dtype, df.shape[0], df[i].isna().sum(), round((df[i].isna().sum()/df.shape[0])*100, 2), df[i].nunique(), df[i].unique()[:5]])

df_decs = pd.DataFrame(ListofDecs, columns=['column', 'type', 'total row', 'null', 'percentage null', 'number of unique value', 'sample unique value'])
df_decs

"""Setelah dilakukan perubahan tipe data pada kolom `Financial Stress`, ditemukan 3 nilai null pada kolom tersebut. Oleh karena itu, baris yang mengandung nilai null akan dihapus untuk menjaga keaslian data.

# Exploratory Data Analysis

## Univariate Analysis
"""

cat = df.select_dtypes('object').columns
num = df.select_dtypes('number').columns.drop('Depression')

print(f'Kolom dengan tipe Kategori: \n{cat}\n')
print(f'Kolom dengan tipe Numerik: \n{num}')

import matplotlib.pyplot as plt

# Set figure
plt.figure(figsize=(8,8))

# Pie plot
plt.pie(
    df['Depression'].value_counts(),
    labels=df['Depression'].value_counts().index.map({0:'No ', 1:'Yes'}),
    autopct='%1.1f%%',
    textprops={'fontsize': 14, 'fontweight': 'bold'},
    colors=['#0786ff', '#ff9999'],
    startangle=90,
    explode=(0.05, 0),
)

# Judul
plt.title('Distribusi Depression', fontsize=18, fontweight='bold')

# Menjaga bentuk pie bulat
plt.axis('equal')

# Tampilkan plot
plt.show()

"""Distribusi pada data target (Depression) sedikit **imbalance** (tidak seimbang), yang mungkin dapat menyebabkan model cenderung memprediksi kelas mayoritas. Oleh karena itu, untuk mengantisipasi permasalahan ini, akan dilakukan percobaan menggunakan teknik **oversampling** pada **data train** (*setelah proses pembagian data menjadi data latih dan data uji*)."""

# Melakukan visualisasi countplot terhadap kolom kategori
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(cat):
  plt.subplot(2, 4, i+1)
  ax = sns.countplot(data=df, y=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

"""Berdasarkan distribusi nilai pada kolom-kolom kategorikal, ditemukan bahwa kolom `City` dan `Profession` memiliki beberapa kategori dengan jumlah data yang sangat sedikit (dominan pada satu kategori saja). Selain itu, kolom City juga memiliki terlalu banyak kategori, yang ***dapat menyebabkan curse of dimensionality***. Oleh karena itu, kedua kolom tersebut akan dihapus dari dataset.

Selain itu, pada kolom `Sleep Duration`, `Dietary Habits`, dan `Degree`, terdapat kategori bernilai "Others" yang tidak merepresentasikan informasi yang jelas serta jumlahnya sangat sedikit. Maka dari itu, baris data yang memiliki nilai "Others" pada fitur-fitur tersebut akan dihapus dari dataset.
"""

# Melakukan visualisasi histogram terhadap kolom numerik
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(2, 4, i+1)
  sns.histplot(data=df, x=col, color='blue', kde=True)
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

# Melakukan visualisasi boxplot terhadap kolom numerik
plt.figure(figsize=(20,10))

plt.style.use('ggplot')

for i, col in enumerate(num):
  plt.subplot(2, 4, i+1)
  sns.boxplot(data=df, x=col, color='blue')
  plt.title(col, fontsize=15, weight='bold', usetex=False)

plt.tight_layout()
plt.show()

""" Kolom `Work Pressure` dan `Job Satisfaction` juga menunjukkan dominasi pada satu nilai tertentu, sehingga tidak memberikan variasi yang signifikan untuk analisis. Oleh karena itu, kedua kolom tersebut akan dihapus dari dataset.  

 Sementara itu, kolom `Age` dan `CGPA` teridentifikasi memiliki nilai outlier yang dapat memengaruhi hasil analisis. Outlier pada kedua kolom tersebut akan dihapus pada tahap praproses selanjutnya.
"""

# menampilkan korelasi antara feature yg ada
plt.figure(figsize=(10,10))
sns.heatmap(df[num].corr(), annot=True, fmt='.2f')
plt.tight_layout()
plt.title('Correlation Between Feature Numeric', weight='bold')
plt.show()

"""Apabila dilihat berdasarkan korelasi antara feature numerik yang dimiliki, pada dataset ini tidak terdapat feature redundant"""

# Menghapus kolom & nilai yang tidak diperlukan
df.drop(['City', 'Profession', 'Job Satisfaction', 'Work Pressure'], axis=1, inplace=True)

df = df[~((df['Sleep Duration'] == 'Others') |
        (df['Dietary Habits'] == 'Others') |
        (df['Degree'] == 'Others') |
        (df['Financial Stress'] == 7))]

df.shape

"""## Multivariate Analysis"""

# membuat function untuk melakukan visualisasi persentase setiap values
def plot_distribution_percentage_category(col, orient):

  def percentage_feature(col):
    df_perce = df.groupby([col, 'Depression']).size().reset_index().rename(columns={0:'Count'})
    df_perce['Percentage'] = ((df_perce['Count']/df_perce.groupby(col)['Count'].transform('sum'))*100).round(2)
    df_perce.sort_values('Percentage', ascending= False, inplace=True)
    return df_perce

  data = percentage_feature(col)

  if orient == 'h':
    plt.figure(figsize=(12, 8))
    ax = sns.barplot(y=col, x='Percentage', hue='Depression', data=data, orient=orient, palette="mako")
    for p in ax.patches:
        width = p.get_width()
        ax.annotate(f'{width:.2f}%',
                    xy=(width / 2, p.get_y() + p.get_height() / 2),
                    ha='center', va='center', color='white', fontsize=10, fontweight='bold')

    plt.xlabel('Percentage by Total Person', fontsize=12)

  else:
    plt.figure(figsize=(12, 8))
    ax =sns.barplot(data=data, x=col, y='Percentage', hue='Depression', palette='mako')
    for p in ax.patches:
      height = p.get_height()
      ax.annotate(f'{height:.2f}%',
                  xy=(p.get_x() + p.get_width() / 2, height/2),
                  ha='center', va='bottom', fontsize=10, color='white', fontweight='bold')

    plt.ylabel('Percentage by Total Person', fontsize=12)

  plt.title(f"Percentage of Overall {col} Distribution", fontsize=14, fontweight='bold')
  plt.legend(title='Depression', fontsize=10, title_fontsize=12)
  plt.show()

plot_distribution_percentage_category('Gender', 'v')

"""Berdasarkan kategori **Gender**, tingkat kemungkinan mengalami depresi antara laki-laki dan perempuan menunjukkan persentase yang hampir sama, yaitu ***masing-masing sebesar 58%***.

Hal ini dapat dilihat pada diagram di bawah, yang memperlihatkan distribusi persentase depresi berdasarkan gender.
"""

plot_distribution_percentage_category('Sleep Duration', 'h')

"""Grafik menunjukkan bahwa semakin pendek durasi tidur, semakin tinggi persentase individu yang mengalami depresi, dengan angka tertinggi pada kelompok tidur kurang dari 5 jam (64,54%). Meskipun proporsi depresi menurun seiring bertambahnya durasi tidur, tidur lebih dari 8 jam tidak menunjukkan perbedaan signifikan. Ini menunjukkan bahwa durasi tidur berpengaruh terhadap depresi, namun bukan satu-satunya faktor."""

plot_distribution_percentage_category('Dietary Habits', 'v')

"""Grafik menunjukkan bahwa individu dengan pola makan tidak sehat memiliki persentase depresi tertinggi (70,75%), sedangkan pada pola makan sehat, persentase depresi lebih rendah (45,37%). Hal ini menunjukkan bahwa semakin baik pola makan seseorang, semakin rendah kecenderungan mengalami depresi."""

plot_distribution_percentage_category('Have you ever had suicidal thoughts ?', 'v')

"""Pemikiran untuk bunuh diri merupakan tanda serius gangguan mental, terutama depresi. Data menunjukkan bahwa 79% dari mereka yang pernah memiliki pikiran tersebut mengalami depresi, jauh lebih tinggi dibandingkan 23% pada mereka yang tidak pernah berpikiran demikian. Ini menegaskan kuatnya kaitan antara pikiran bunuh diri dan depresi."""

plot_distribution_percentage_category('Academic Pressure', 'v')

"""Grafik menunjukkan bahwa semakin tinggi tingkat tekanan akademik, semakin besar persentase individu yang mengalami depresi. Pada tingkat tekanan tertinggi (skor 5), sebanyak **86,06%** mengalami depresi, sedangkan pada tingkat tekanan terendah (skor 1), hanya **19,39%** yang mengalami depresi. Hal ini mengindikasikan bahwa tekanan akademik yang tinggi memiliki kaitan kuat dengan meningkatnya risiko depresi."""

plot_distribution_percentage_category('Financial Stress', 'h')

"""Diagram ini menunjukkan tren penurunan persentase stres finansial pelajar seiring naiknya kategori (1.0-5.0). Pada kategori tertinggi (1.0), 68,13% pelajar mengalami stres finansial, namun angka ini turun drastis menjadi hanya 18,72% di kategori terendah (5.0). Sebaliknya, persentase pelajar tanpa stres finansial meningkat dari 31,87% (kategori 1.0) menjadi 81,28% (kategori 5.0), menunjukkan bahwa kategori lebih tinggi berkorelasi dengan stabilitas keuangan yang lebih baik. Data "Percentage by Total Customers" tidak tersedia untuk analisis lebih lanjut."""

def plot_distribution_numeric(data, col):
  plt.figure(figsize=(10,6))
  sns.histplot(data=data, x=col, hue='Depression', kde=True)

  mean_all = data[col].mean()
  mean_Depression = data[data['Depression'] == 1][col].mean()
  mean_not_Depression = data[data['Depression'] == 0][col].mean()

  plt.axvline(mean_all, color='#28B463', linestyle='dashed', label=f'Rata-rata keseluruhan {mean_all:.2f}')
  plt.axvline(mean_Depression, color='#2E86C1', linestyle='dotted', label=f'Rata-rata Pelajar Depression {mean_Depression:.2f}')
  plt.axvline(mean_not_Depression, color='#FF5733', linestyle='solid', label=f'Rata-rata Pelajar tidak Depression {mean_not_Depression:.2f}')

  plt.text(mean_all, plt.ylim()[1]*0.9, color='#28B463', s=f'Rata-rata keseluruhan\n{mean_all:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
  plt.text(mean_Depression, plt.ylim()[1]*0.8, color='#2E86C1', s=f'Rata-rata Pelajar Depression\n{mean_Depression:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))
  plt.text(mean_not_Depression, plt.ylim()[1]*0.7, color='#FF5733', s=f'Rata-rata Pelajar tidak Depression\n{mean_not_Depression:.2f}', ha='left', fontsize=10, bbox=dict(facecolor='white', alpha=0.6))

  plt.title(f'Distribusi {col} Berdasarkan Status Depression Pelajar', fontsize=14)
  plt.xlabel(f'{col}', fontsize=12)
  plt.ylabel('Total Pelajar', fontsize=12)
  plt.tight_layout()
  plt.show()

plot_distribution_numeric(df, 'Age')

"""Grafik menunjukkan bahwa customer dengan status depresi cenderung berusia lebih muda, dengan rata-rata usia 24 tahun, dibandingkan yang tidak depresi dengan rata-rata 27tahun. Hal ini mengindikasikan bahwa depresi lebih banyak dialami oleh kelompok usia muda."""

plot_distribution_numeric(df, 'CGPA')

"""Perbandingan CGPA antara pelajar yang mengalami depresi dan yang tidak menunjukkan tidak adanya perbedaan signifikan, dengan keduanya memiliki rata-rata sekitar 7,6. Hal ini mengindikasikan bahwa meskipun mengalami tekanan psikologis, pelajar dengan depresi tetap mampu menjaga performa akademiknya. Namun, kestabilan nilai ini tidak serta-merta mencerminkan kondisi mental yang sehat, melainkan bisa menjadi tanda bahwa mereka menekan emosinya demi mempertahankan pencapaian."""

plot_distribution_numeric(df, 'Study Satisfaction')

"""Rata-rata kepuasan studi secara keseluruhan adalah 2.94, sedangkan pelajar tanpa depresi (0) memiliki rata-rata 2.75 dan pelajar dengan depresi (1) memiliki rata-rata lebih tinggi, yaitu 3.21. Data menunjukkan bahwa pelajar dengan depresi cenderung melaporkan tingkat kepuasan studi yang sedikit lebih tinggi dibandingkan pelajar tanpa depresi."""

plot_distribution_numeric(df, 'Work/Study Hours')

"""Grafik menunjukkan bahwa customer dengan depresi memiliki rata-rata jam kerja/belajar lebih tinggi (7,81 jam) dibandingkan yang tidak depresi (6,24 jam). Ini mengindikasikan bahwa semakin banyak jam kerja/belajar, potensi mengalami depresi cenderung meningkat.

# Data Preprocessing
"""

df_prep = df.copy()

"""Dataset disalin ke dalam variabel `df_prep` untuk melakukan preprocessing, sehingga proses tersebut tidak memengaruhi dataset asli `df`.

## Handle Missing Values
"""

df_prep.isnull().sum()

"""Pada tahap ini dikarenakan missing value hanya terdapat 3 value pada kolom `Financial Stress` maka kita akan menghapus kolom tersebut. Karena hanya terdapat 3 value pada kolom tersebut maka kita akan menghapus kolom tersebut, dikarenakan agar menjaga keaslian data."""

df_prep = df_prep.dropna()

df_prep.isnull().sum()

"""## Handle Duplicate Values"""

# Handle Duplicate Value
(f'Jumlah Data Duplikat: {df_prep.duplicated().sum()}')

"""Saat dilakukan pengecekan, tidak ditemukan data duplikat dalam dataset. Hal ini menunjukkan bahwa setiap entri dalam dataset bersifat unik, sehingga tidak ada data yang berulang yang dapat memengaruhi hasil analisis modeling kedepannya

## Handle Outlier Values
"""

col_selected = ['Age', 'CGPA']

def remove_outliers(df, columns):
    df_cleaned = df.copy()
    for col in columns:
        Q1 = df_cleaned[col].quantile(0.25)
        Q3 = df_cleaned[col].quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        df_cleaned = df_cleaned[~((df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound))]
    return df_cleaned

df_prep = remove_outliers(df_prep, col_selected)

"""Untuk menangani outlier, kali ini akan dilakukan penghapusan outlier pada kolom `Age` dan `CGPA` menggunakan metode IQR. Metode ini digunakan untuk menghilangkan nilai-nilai yang berada di luar batas bawah dan batas atas yang ditentukan, sehingga data menjadi lebih bersih dan representatif.

## Encoding Values
"""

df_prep.head()

"""Encoding Fitur Kategorikal dilakukan 2 bagian, yakni:

1. *Label Encoding* yaitu, mengonversi nilai kategori menjadi angka integer (`0` dan `1`). Variabel yang akan diproses yakni:  <br>
    `Gender`, `Have you ever had suicidal thoughts ?`, `Family History of Mental Illness`
2. *One Hot Ecoding* yaitu mengubah setiap kategori menjadi kolom biner terpisah untuk data tidak terurut. Variabel yang akan diproses yakni: <br>
    `Sleep Duration`, `Dietary Habits`, `Degree`
"""

from sklearn.preprocessing import OneHotEncoder, LabelEncoder

# Label Encoding untuk 'Gender'
col_label = ['Gender', 'Have you ever had suicidal thoughts ?', 'Family History of Mental Illness']
for col in col_label:
  df_prep[col] = LabelEncoder().fit_transform(df_prep[col])

# One Hot Encoding untuk kolom yang disebutkan
one_hot_columns =  ['Sleep Duration', 'Dietary Habits', 'Degree']
df_prep = pd.get_dummies(df_prep, columns=one_hot_columns, dtype=int)

df_prep.head()

df_prep.shape

"""setelah dilakukannya encoding jumlah kolom bertambah menjadi 44 kolom

## Spliting Data
"""

from sklearn.model_selection import train_test_split

X = df_prep.drop('Depression', axis=1)
y = df_prep['Depression']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)

"""Membagi data dengan proporsi 80:20. Dengan 80% digunakan untuk training model dan 20% digunakan untuk testing model."""

print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of X_test: {X_test.shape}')

"""## Transformation Values"""

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""Melakukan scaling value dengam MinMaxScaler untuk menyamaratakan scalar dari setiap fitur.

## Handle Imbalance Data
"""

print(f'Number of Depression Before SMOTE: {y_train.value_counts()[1]}')
print(f'Number of Not Depression Loan Before SMOTE: {y_train.value_counts()[0]}')

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_smt, y_train_smt = smote.fit_resample(X_train, y_train)

print(f'Number of Depression After SMOTE: {y_train_smt.value_counts()[1]}')
print(f'Number of Not Depression After SMOTE: {y_train_smt.value_counts()[0]}')

"""SMOTE digunakan untuk mengatasi ketidakseimbangan kelas pada data latih. Pengujian juga dilakukan pada data tanpa SMOTE untuk membandingkan akurasi dan menilai efektivitas metode tersebut.

# Model Development

Pada tahap ini dilakukannya pengujian beberapa algoritma klassifikasi diantarannya `Logistic Regression`, `Decision Tree`, `Random Forest`, `Ada Boost`, `SVC`, `KNeighbors Classifier`, `Gaussian NB`, & `XGBoost` untuk mengetahui model mana yang memiliki performa terbaik dalam menangani kasus ini
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

"""Dibuat sebuah fungsi untuk melatih model menggunakan beberapa algoritma klasifikasi sekaligus, serta mengukur skor performanya pada data training dan data testing. Perbandingan antara kedua skor ini bertujuan untuk mengevaluasi potensi terjadinya overfitting maupun underfitting pada masing-masing model"""

def train_model(X_train, y_train, X_test, y_test):
  # list model
  models ={
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42),
    'Ada Boost': AdaBoostClassifier(random_state=42),
    'SVC': SVC(random_state=42),
    'KNeigbors': KNeighborsClassifier(),
    'Gaussian NB': GaussianNB(),
    'XGBoost': XGBClassifier(random_state=42)
    }


  result = []
  for nama_model, model in models.items():
    model.fit(X_train, y_train)
    y_pred_test = model.predict(X_test)
    y_pred_train = model.predict(X_train)


    # data train
    acc_train = round(accuracy_score(y_train, y_pred_train),2)
    prec_train = round(precision_score(y_train, y_pred_train),2)
    rec_train = round(recall_score(y_train, y_pred_train),2)
    f1_train = round(f1_score(y_train, y_pred_train),2)
    roc_train = round(roc_auc_score(y_train, y_pred_train),2)

    # data test
    acc_test = round(accuracy_score(y_test, y_pred_test),2)
    prec_test = round(precision_score(y_test, y_pred_test),2)
    rec_test = round(recall_score(y_test, y_pred_test),2)
    f1_test = round(f1_score(y_test, y_pred_test),2)
    roc_test = round(roc_auc_score(y_test, y_pred_test),2)

    # result
    result.append([nama_model, acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
  result_df = pd.DataFrame(result, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
  return result_df

result = train_model(X_train_smt, y_train_smt, X_test, y_test)
print('With SMOTE')
result

"""Setelah dilakukan pengujian pada data yang telah diproses menggunakan SMOTE, diperoleh hasil skor dari beberapa algoritma klasifikasi. Algoritma `Decision Tree` dan `Random Forest` menunjukkan indikasi overfitting, ditandai dengan adanya selisih skor yang cukup besar antara data train dan test. Sementara itu, algoritma lainnya menunjukkan performa yang relatif seimbang. Untuk itu, pengujian lebih lanjut akan difokuskan pada dua algoritma dengan skor terbaik, yaitu `XGBoost` dan `Logistic Regression`"""

result = train_model(X_train, y_train, X_test, y_test)
print('Without SMOTE')
result

"""Hasil pengujian tanpa menggunakan SMOTE menunjukkan skor yang sangat mirip dengan hasil pengujian menggunakan SMOTE. Oleh karena itu, pada tahap selanjutnya akan digunakan data tanpa SMOTE untuk menjaga keaslian dan keorisinilan data yang ada.

## XGBoost & Hyperparamater Tuning
"""

from xgboost import XGBClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Parameter grid untuk XGBoost
param_grid = {
    'n_estimators': [50, 100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1, 0.2],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'gamma': [0, 1, 5],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [1, 1.5, 2]
}

# Inisialisasi XGBoost
xgb = XGBClassifier(eval_metric='logloss', random_state=42)

# RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb,
    param_distributions=param_grid,
    n_iter=50,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    random_state=42
)

# Fit model
random_search.fit(X_train, y_train)

# Parameter terbaik
best_params_XG = random_search.best_params_
print(f"Best parameters: {best_params_XG}")

# Model dengan parameter terbaik
model_tuning_XG = random_search.best_estimator_
model_tuning_XG.fit(X_train, y_train)

# Prediksi
y_pred_test = model_tuning_XG.predict(X_test)
y_pred_train = model_tuning_XG.predict(X_train)

# Evaluasi train
acc_train = round(accuracy_score(y_train, y_pred_train), 2)
prec_train = round(precision_score(y_train, y_pred_train), 2)
rec_train = round(recall_score(y_train, y_pred_train), 2)
f1_train = round(f1_score(y_train, y_pred_train), 2)
roc_train = round(roc_auc_score(y_train, y_pred_train), 2)

# Evaluasi test
acc_test = round(accuracy_score(y_test, y_pred_test), 2)
prec_test = round(precision_score(y_test, y_pred_test), 2)
rec_test = round(recall_score(y_test, y_pred_test), 2)
f1_test = round(f1_score(y_test, y_pred_test), 2)
roc_test = round(roc_auc_score(y_test, y_pred_test), 2)

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix XGBoost (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

# Hasil akhir
result_tuning = []
result_tuning.append(['XGBoost(Tuning)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_tuning = pd.DataFrame(result_tuning, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_tuning

from sklearn.model_selection import cross_validate
import numpy as np

# Melakukan cross validation pada model terbaik dari RandomizedSearch
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

cv_results = cross_validate(
    model_tuning_XG,           # Model terbaik hasil tuning
    X_test,                # Data training
    y_test,                # Label training
    cv=5,                   # 5-fold CV
    scoring=scoring,
    return_train_score=True,
    n_jobs=-1
)

# Tampilkan hasil rata-rata CV
print("Cross Validation Scores (mean of 5 folds):")
for metric in scoring:
    mean_score = np.mean(cv_results[f'test_{metric}'])
    print(f"{metric}: {mean_score:.2f}")

"""## Logistic Regressio & Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = [
    # liblinear: hanya l1 dan l2
    {
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'max_iter': [100, 200, 500, 1000],
    },
    # lbfgs: hanya l2 dan None
    {
        'penalty': ['l2', None],
        'solver': ['lbfgs'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'max_iter': [100, 200, 500, 1000],
    },
    # saga: bisa l1, l2, elasticnet, None
    {
        'penalty': ['l1', 'l2', 'elasticnet', None],
        'solver': ['saga'],
        'C': [0.001, 0.01, 0.1, 1, 10, 100],
        'l1_ratio': [0.1, 0.5, 0.9],  # hanya dipakai kalau penalty == 'elasticnet'
        'max_iter': [100, 200, 500, 1000],
    },
]

# Initialize Logistic Regression
Logistic = LogisticRegression(random_state=42)

# Initialize GridSearchCV
random_search = RandomizedSearchCV(
    estimator=Logistic,
    param_distributions=param_grid,
    n_iter=10,               # Jumlah iterasi (kombinasi) untuk dicoba
    cv=3,                    # Cross-validation 5 fold
    scoring='roc_auc',       # Metode evaluasi ROC AUC
    n_jobs=-1,               # Paralel untuk mempercepat
    random_state=42          # Seed untuk hasil yang konsisten
)

# Fit the grid search to the data
random_search.fit(X_train, y_train)

# Get the best parameters and best score
best_params_LR = random_search.best_params_

print(f"Best parameters: {best_params_LR}")

# Train the model with the best parameters
model_tuning_LR = random_search.best_estimator_
model_tuning_LR.fit(X_train, y_train)

# Evaluate the model (example using roc_auc_score)
result_tuning = []

y_pred_test = model_tuning_LR.predict(X_test)
y_pred_train = model_tuning_LR.predict(X_train)

# data train
acc_train = round(accuracy_score(y_train, y_pred_train),2)
prec_train = round(precision_score(y_train, y_pred_train),2)
rec_train = round(recall_score(y_train, y_pred_train),2)
f1_train = round(f1_score(y_train, y_pred_train),2)
roc_train = round(roc_auc_score(y_train, y_pred_train),2)

# data test
acc_test = round(accuracy_score(y_test, y_pred_test),2)
prec_test = round(precision_score(y_test, y_pred_test),2)
rec_test = round(recall_score(y_test, y_pred_test),2)
f1_test = round(f1_score(y_test, y_pred_test),2)
roc_test = round(roc_auc_score(y_test, y_pred_test),2)

# showing confusion metrics
cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix Logistic Regression (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

result_tuning.append(['LogisticRegression(Tuning)', acc_train, acc_test, prec_train, prec_test, rec_train, rec_test, f1_train, f1_test, roc_train, roc_test])
result_tuning = pd.DataFrame(result_tuning, columns=['model', 'acc_train', 'acc_test', 'prec_train', 'prec_test', 'rec_train', 'rec_test', 'f1_train', 'f1_test', 'roc_train', 'roc_test'])
result_tuning

from sklearn.model_selection import cross_validate
import numpy as np

# Melakukan cross validation pada model terbaik dari RandomizedSearch
scoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']

cv_results = cross_validate(
    model_tuning_LR,           # Model terbaik hasil tuning
    X_test,                # Data training
    y_test,                # Label training
    cv=5,                   # 5-fold CV
    scoring=scoring,
    return_train_score=True,
    n_jobs=-1
)

# Tampilkan hasil rata-rata CV
print("Cross Validation Scores (mean of 5 folds):")
for metric in scoring:
    mean_score = np.mean(cv_results[f'test_{metric}'])
    print(f"{metric}: {mean_score:.2f}")

"""# Evaluation

Algoritma yang dipilih sebagai model dalam pembuatan machine learning ini adalah `XGBoost`. Pada tahap evaluasi, metrik yang difokuskan adalah ***F1-score*** dan ***ROC-AUC*** score. Pemilihan *F1-score dilakukan karena metrik ini mampu menangani ketidakseimbangan data dengan menyeimbangkan antara presisi dan recall*, sedangkan *ROC-AUC dipilih untuk mengukur kemampuan model dalam membedakan kelas secara keseluruhan, terutama pada kasus klasifikasi biner seperti ini*.
"""

y_pred_test = model_tuning_LR.predict(X_test)
f1_test = round(f1_score(y_test, y_pred_test),2)
roc_test = round(roc_auc_score(y_test, y_pred_test),2)
resume = classification_report(y_test, y_pred_test, target_names=['Not Depression', 'Depression'])

print(f'F1-Score: {f1_test}')
print(f'ROC-AUC Score: {roc_test}')
print(f'Classification Report:\n{resume}')

cm = confusion_matrix(y_test, y_pred_test)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix Logistic Regression (Tuning)')
plt.xlabel('Predicted')
plt.ylabel('Actual')

"""# Feature Importance"""

import shap

# Assuming 'model_tuning_XG' is your best XGBoost model
explainer = shap.Explainer(model_tuning_XG, X_train)
shap_values = explainer(X_test)

# Visualize feature importance
shap.summary_plot(shap_values, X_test, plot_type="bar", feature_names=X.columns, max_display=7)
shap.summary_plot(shap_values, X_test, feature_names=X.columns, max_display=7)

"""**Rekomendasi Strategis:**

- Pikiran Bunuh Diri
Buat sistem deteksi dini dan layanan konseling prioritas.
- Tekanan Akademik
Evaluasi beban studi & adakan pelatihan manajemen stres.
- Stres Finansial
Tawarkan beasiswa, konseling keuangan, dan kerja paruh waktu.
- Usia Muda (Remaja Awal)
Fokus intervensi pada mahasiswa baru dan usia 18–21 tahun.
- Jam Belajar/Bekerja Tinggi
Atur beban studi agar seimbang, dorong waktu istirahat.
- Pola Hidup Tidak Sehat
Menjaga pola hidup sehat dengan mengadakan kegiatan olahraga ditiap minggunya.
- Kepuasan Studi
Tingkatkan kualitas pembelajaran & perhatikan feedback mahasiswa.
"""